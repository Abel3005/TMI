{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import subprocess, os\n",
    "from time import gmtime, strftime\n",
    "import re, sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def log(msg, flag=None, path=\"./logs\"):\n",
    "    if flag==None:\n",
    "        flag = 0\n",
    "    head = [\"DEBUG\", \"ERROR\", \"WARN\", \"STATUS\", \"INFO\"]\n",
    "    now = strftime(\"%H:%M:%S\", gmtime())\n",
    "    \n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    if not os.path.isfile(f\"{path}/{head[flag]}.log\"):\n",
    "        assert subprocess.call(f\"echo \\\"[{now}][{head[flag]}] > {msg}\\\" > {path}/{head[flag]}.log\", shell=True)==0, print(f\"[ERROR] > shell command failed to execute\")\n",
    "    else: assert subprocess.call(f\"echo \\\"[{now}][{head[flag]}] > {msg}\\\" >> {path}/{head[flag]}.log\", shell=True)==0, print(f\"[ERROR] > shell command failed to execute\")\n",
    "\n",
    "def get_time():\n",
    "    return strftime(\"%Y-%m-%d_%H%M%S\", gmtime())\n",
    "\n",
    "\n",
    "class jobkorea:\n",
    "    header = {\"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"}\n",
    "    post_header= {\"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\",\"X-Requested-With\":\"XMLHttpRequest\"}\n",
    "    all_dict = {}\n",
    "    query_log = []\n",
    "    error_jobid_list = []\n",
    "    def get_job(self, query='duty=1000229%2C1000230%2C1000231%2C1000232%2C1000233%2C1000234%2C1000235%2C1000236%2C1000237%2C1000238%2C1000239%2C1000240%2C1000241%2C1000242%2C1000243%2C1000244%2C1000245%2C1000246&sort=6',flag='daily'):\n",
    "        if flag == 'all':\n",
    "            base_url = f'https://m.jobkorea.co.kr/Recruit/JobList/arealist?{query}&page=1'\n",
    "            response = self.get_url(base_url)\n",
    "            if response:\n",
    "                soup = BeautifulSoup(response.text)\n",
    "                self.query_log.append(query)\n",
    "            else:\n",
    "                log(f\"Don't get jobs with this query: {query}\",1)\n",
    "                return\n",
    "            all_num = int(soup.find(\"div\",attrs={'id':\"devNormalListContainer\"})['data-agicnt'])\n",
    "            response.close()\n",
    "            del soup\n",
    "        if flag == 'daily':\n",
    "            all_num = 100\n",
    "        log(f\"query({query} include {all_num} jobs)\",4)\n",
    "        pagenum = (all_num //40) + 1\n",
    "        for p in range(1,pagenum+1):\n",
    "            target_url = f'https://m.jobkorea.co.kr/Recruit/JobList/arealist?page={p}&sort=6'\n",
    "            response = self.get_url(target_url)\n",
    "            if response:\n",
    "                soup = BeautifulSoup(response.text)\n",
    "                _list = soup.find(\"div\",attrs={\"class\":\"list list-recruit list-recruit-badge\"}).find_all('li')\n",
    "                for e in _list:\n",
    "                    job_id = re.compile(\"[0-9]+\").findall(e.find('a')['href'])[0]\n",
    "                    _dict = {}\n",
    "                    _dict[\"company\"] = e.find('div',attrs={\"class\":\"company\"}).text \n",
    "                    _dict[\"title\"] = e.find('div',attrs={\"class\":\"title\"}).text\n",
    "                    self.all_dict[job_id] = _dict\n",
    "                    # self.get_giread(job_id)\n",
    "                response.close()\n",
    "                del soup\n",
    "            else:\n",
    "                log(f\"{target_url} error\",4)\n",
    "            \n",
    "        for job_id in self.all_dict:\n",
    "            self.all_dict[job_id]['job_id'] = job_id\n",
    "            self.post_swipgegiread(job_id)  \n",
    "\n",
    "                \n",
    "\n",
    "    def post_swipgegiread(self,_number):\n",
    "        target_url = f'https://m.jobkorea.co.kr/Recruit/SwipeGIReadInfo/{_number}'\n",
    "        response = self.post_url(target_url)\n",
    "        if not response:\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        tmp = soup.find('div',attrs={\"id\":\"rowReceipt\"})\n",
    "        for e in tmp.find_all('div',attrs={\"class\":\"receiptTermDate\"}):\n",
    "            self.all_dict[_number][e.find('div',attrs={\"class\":\"badge\"}).text] = e.find('div',attrs={\"class\":\"date\"}).text\n",
    "        for e in tmp.find_all('div',attrs={'class':'field'}):\n",
    "            self.all_dict[_number][e.find('div',attrs={'class':'label'}).text.strip()] = e.find('div',attrs={'class':'value'}).text.strip()\n",
    "        tmp = soup.find('div',attrs={\"id\":\"rowGuidelines\"})\n",
    "        for e in tmp.find_all('div',attrs={'class':'field'}):\n",
    "            self.all_dict[_number][e.find('div',attrs={'class':'label'}).text.strip()] = e.find('div',attrs={'class':'value'}).text.strip()\n",
    "        tmp = soup.find('div',attrs={\"id\":\"rowCompany\"})\n",
    "        for e in tmp.find_all('div',attrs={'class':'field'}):\n",
    "            self.all_dict[_number][e.find('div',attrs={'class':'label'}).text.strip()] = e.find('div',attrs={'class':'value'}).text.strip()\n",
    "        response.close()\n",
    "        del soup\n",
    "\n",
    "            \n",
    "    def get_giread(self,_number):\n",
    "        response = self.get_url(f\"https://www.jobkorea.co.kr/Recruit/GI_Read/{_number}\")\n",
    "        soup = BeautifulSoup(response.text)\n",
    "        if not response and not soup.find('meta',attrs={\"name\":\"description\"}):\n",
    "            return\n",
    "        self.all_dict[_number]['description']=soup.find('meta',attrs={\"name\":\"description\"})\n",
    "        self.all_dict[_number]['keywards'] = soup.find('meta',attrs={\"name\":\"keywords\"})\n",
    "        response.close()\n",
    "        del soup\n",
    "\n",
    "    def get_url(self,url):\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            r = requests.get(url,headers=self.header, timeout=3)\n",
    "        except Exception as e:\n",
    "            #log(f\"request get {url} error {e}\",1)\n",
    "            return None \n",
    "        else:\n",
    "            #log(f\"request get : {url} status_conde: {r.status_code}\",4)\n",
    "            return r\n",
    "    def post_url(self,url):\n",
    "        try:\n",
    "            r = requests.post(url,headers=self.post_header,timeout=3)\n",
    "        except Exception as e:\n",
    "            #log(f\"request post {url} error {e}\",1)\n",
    "            return None \n",
    "        else:\n",
    "            log(f\"request post :{url} status_conde: {r.status_code}\",4)\n",
    "            return r\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        _list = []\n",
    "        for i in self.all_dict:\n",
    "            _list.append(self.all_dict[i])\n",
    "        return pd.DataFrame(_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = jobkorea()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.get_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= tmp.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['company', 'title', 'job_id', '시작', '마감', '접수방법', '이력서', '모집분야', '경력',\n",
       "       '학력', '고용형태', '급여', '근무지역', '근무시간', '모집인원', '산업', '사원수', '기업구분', '자본금',\n",
       "       '설립일', '매출액', '대표자', '주요사업', '직급/직책', '급여\\r\\n                    월급',\n",
       "       '급여\\r\\n                    연봉', '인사담당자', '전화번호', '휴대전화', 'E-mail',\n",
       "       '홈페이지', '우대조건', '핵심역량', '스킬', '급여\\r\\n                    일급', '첨부희망',\n",
       "       '급여\\r\\n                    시급'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "_list = []\n",
    "for i in tmp.all_dict:\n",
    "    tmp.all_dict[i]['jobid'] = i\n",
    "    _list.append(tmp.all_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['company', 'title', '시작', '마감', '접수방법', '모집분야', '경력', '학력', '고용형태',\n",
       "       '급여', '근무지역', '우대조건', '스킬', '핵심역량', '모집인원', '산업', '사원수', '기업구분', '설립일',\n",
       "       '대표자', '주요사업', 'jobid', '인사담당자', '전화번호', '휴대전화', '직급/직책', '근무시간', '자본금',\n",
       "       '매출액', 'E-mail', '홈페이지', '이력서', '첨부희망', '급여\\r\\n                    연봉',\n",
       "       '급여\\r\\n                    월급', '급여\\r\\n                    시급', '사전인터뷰',\n",
       "       '급여\\r\\n                    일급'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_json(\"test.json\", orient='records', lines=True, force_ascii=False, date_format='iso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"}\n",
    "post_header= {\"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\",\"X-Requested-With\":\"XMLHttpRequest\"}\n",
    "\n",
    "for i in tmp.all_dict:\n",
    "    r = requests.get(f\"https://www.jobkorea.co.kr/Recruit/GI_Read/{i}\",headers=header)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://m.jobkorea.co.kr/Recruit/SwipeGIReadInfo/45325074'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'https://m.jobkorea.co.kr/Recruit/aSwipeGIReadInfo/{i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1827\n"
     ]
    }
   ],
   "source": [
    "print(len(tmp.all_dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
