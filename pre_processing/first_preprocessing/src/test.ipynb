{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, json, datetime, boto3, redis, sys, farmhash\n",
    "import pandas as pd\n",
    "from json.decoder import JSONDecodeError\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "\n",
    "import utils \n",
    "\n",
    "## key를 전역 변수로 설정\n",
    "path = '/mnt/data/airflow/.KEYS/'  # local file path\n",
    "with open(f'{path}/FIRST_PREPROCESSING_KEY.json', 'r') as f:\n",
    "    key = json.load(f)\n",
    "with open(f'{path}/DATA_SRC_INFO.json', 'r') as f:\n",
    "    storage_info = json.load(f)\n",
    "\n",
    "# 로깅 설정 복원\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler(\"app.log\"), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# S3 세션 전역 변수로 선언\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=key['aws_access_key_id'],\n",
    "    aws_secret_access_key=key['aws_secret_key'],\n",
    "    region_name=key['region']\n",
    ")\n",
    "s3 = session.client('s3')\n",
    "\n",
    "# redis 연결 작업\n",
    "redis_ip = storage_info['redis_conn_info']['ip']\n",
    "redis_port = storage_info['redis_conn_info']['port']\n",
    "redis_sassion = redis.StrictRedis(host=redis_ip, port=redis_port, db=0)\n",
    "\n",
    "# sqs url get\n",
    "target_id_queue_url = storage_info['target_id_sqs_queque_arn']\n",
    "\n",
    "'''\n",
    "S3의 특정 버킷에서 json 형식의 데이터를 가져와\n",
    "lake > archive로 크롤링한 데이터를 이식합니다.\n",
    "'''\n",
    "def import_bucket():\n",
    "    pull_bucket_name = storage_info['pull_bucket_name']\n",
    "    data_archive_bucket_name = storage_info['crawl_data_bucket_name']\n",
    "\n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=pull_bucket_name, Prefix='rocketpunch')\n",
    "        if 'Contents' not in response:\n",
    "            logger.warning(f\"No objects found in the bucket: {pull_bucket_name}\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"Object found in the bucket: {pull_bucket_name}\")\n",
    "        all_data = []\n",
    "        for obj in response['Contents']:\n",
    "            try:\n",
    "                # s3에서 데이터를 가져와 all_data 리스트에 데이터를 담습니다.\n",
    "                s3_response = s3.get_object(Bucket=pull_bucket_name, Key=obj['Key'])\n",
    "                json_context = s3_response['Body'].read().decode('utf-8')\n",
    "                cleaned_text = re.sub(r'[\\r\\u2028\\u2029]+', ' ', json_context)\n",
    "                json_list = [json.loads(line) for line in cleaned_text.strip().splitlines()]\n",
    "                data = pd.DataFrame(json_list)\n",
    "                all_data.append(data)\n",
    "\n",
    "                # 파일 이동 및 삭제\n",
    "                copy_source = {\"Bucket\": pull_bucket_name, \"Key\": obj['Key']}\n",
    "                logger.info(\"Start to Copy data from lake to archive\")\n",
    "                #s3.copy(copy_source, data_archive_bucket_name, obj['Key'])\n",
    "                logger.info(\"Start to Delete data from lake\")\n",
    "                #s3.delete_object(Bucket=pull_bucket_name, Key=obj['Key'])\n",
    "                logger.info(f\"Processed and moved file {obj['Key']}\")\n",
    "\n",
    "            except JSONDecodeError as e:\n",
    "                logger.error(f\"JSONDecodeError while processing {obj['Key']}: {e}\")\n",
    "            except ClientError as e:\n",
    "                logger.error(f\"ClientError while accessing S3: {e}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"An unexpected error occurred while processing {obj['Key']}: {e}\")\n",
    "\n",
    "        # all_data가 있는 경우 pandas concat으로 합친 값을 return 합니다.\n",
    "        # 없는 경우, none 타입으로 return 합니다.\n",
    "        if all_data:\n",
    "            logger.info(f\"Data successfully imported from bucket {pull_bucket_name}\")\n",
    "            return pd.concat(all_data, ignore_index=True)\n",
    "        else:\n",
    "            logger.info(f\"No data to import from bucket {pull_bucket_name}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while importing bucket data: {e}\")\n",
    "        raise\n",
    "\n",
    "''''\n",
    " 시간 변환 함수입니다.\n",
    " preprocessing 함수에서 호출합니다.\n",
    "'''\n",
    "def convert_to_timestamp(date_str):\n",
    "    date_pattern = re.compile(r'\\d{4}.\\d{2}.\\d{2}')\n",
    "    match = date_pattern.match(date_str)\n",
    "    \n",
    "    if match:\n",
    "        yy, mm, dd = match.groups()\n",
    "        date = f\"{yy}.{mm}.{dd}\"\n",
    "        date_obj = datetime.datetime.strptime(date, \"%Y.%m.%d\")\n",
    "        return int(date_obj.timestamp())\n",
    "    \n",
    "    return None\n",
    "\n",
    "'''\n",
    " 전처리를 진행합니다. \n",
    " 규칙은 pre-processing_policy.md의 규칙을 따릅니다.\n",
    "'''\n",
    "def preprocessing(df):\n",
    "    logger.info(\"Preprocessing started\")\n",
    "    processed_list = []\n",
    "    \n",
    "    for i, data in df.iterrows():\n",
    "        try:\n",
    "            processing_dict = {}\n",
    "            if pd.notnull(data['job_task']):\n",
    "                processing_dict['job_tasks'] = ' '.join(\n",
    "                    [item for item in re.sub(r'[^.,/\\-+()\\s\\w]', ' ', re.sub(r'\\\\/', '/', data['job_task'])).split() if item not in ['-', '+']]\n",
    "                )\n",
    "            processing_dict['stacks'] = re.sub(r'\\\\/', '/', data['job_specialties'])\n",
    "            processing_dict['job_requirements'] = ' '.join(\n",
    "                [item for item in re.sub(r'[^.,/\\-+()\\s\\w]', ' ', re.sub(r'\\\\/', '/', data['job_detail'])).split() if item not in ['-', '+']]\n",
    "            )\n",
    "            processing_dict['indurstry_type'] = re.sub(r'\\\\/', '/', data['job_industry'])\n",
    "            \n",
    "            #date parsing\n",
    "            # Process date_start (always convert to timestamp, even if format is wrong or missing)\n",
    "            if data['date_start']:\n",
    "                processing_dict['start_date'] = convert_to_timestamp(data['date_start']) or int(datetime.datetime.now().timestamp())\n",
    "            else:\n",
    "                processing_dict['start_date'] = int(datetime.datetime.now().timestamp())\n",
    "            \n",
    "            # Process date_end (only convert if valid, otherwise set to 'null')\n",
    "            if data['date_end']:\n",
    "                end_timestamp = convert_to_timestamp(data['date_end'])\n",
    "                processing_dict['end_date'] = end_timestamp if end_timestamp is not None else 'null'\n",
    "            else:\n",
    "                processing_dict['end_date'] = 'null'    \n",
    "\n",
    "            processing_dict['required_career'] = \"신입\" in data['job_career']\n",
    "            processing_dict['site_symbol'] = \"RP\"\n",
    "            processing_dict['crawl_url'] = data['job_url']\n",
    "            processing_dict['crawl_domain'] = data['crawl_domain']\n",
    "            processing_dict['job_title'] = data['job_title']\n",
    "            processing_dict['company_name'] = data['company_name']\n",
    "\n",
    "            id = farmhash.Fingerprint32(\"RP\" + data['company_name'] + str(data['job_id']))\n",
    "            processing_dict['id'] = id\n",
    "\n",
    "            dt = datetime.datetime.strptime(data['timestamp'], \"%Y-%m-%d_%H:%M:%S\")\n",
    "            processing_dict['get_date'] = int(dt.timestamp())\n",
    "\n",
    "            processed_list.append(processing_dict) \n",
    "            logger.info(f\"Successfully processed data row {i}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"occurred during preprocessing row {i}, id: {processing_dict['id']} : {e}\")\n",
    "    \n",
    "    return pd.DataFrame(processed_list)\n",
    "\n",
    "\n",
    "'''\n",
    " 데이터프레임의 데이터를 DynamoDB에 업로드합니다.\n",
    "'''\n",
    "def upload_data(records):\n",
    "    dynamodb = boto3.resource(\n",
    "        'dynamodb',\n",
    "        aws_access_key_id=key['aws_access_key_id'],\n",
    "        aws_secret_access_key=key['aws_secret_key'],\n",
    "        region_name=key['region']\n",
    "    )\n",
    "    table = dynamodb.Table(storage_info['restore_table_name'])\n",
    "    with table.batch_writer() as batch:\n",
    "        for item in records:\n",
    "            batch.put_item(Item=item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 17:35:59,405 - INFO - Object found in the bucket: crawl-data-lake\n",
      "2024-09-12 17:35:59,429 - INFO - Start to Copy data from lake to archive\n",
      "2024-09-12 17:35:59,430 - INFO - Start to Delete data from lake\n",
      "2024-09-12 17:35:59,431 - INFO - Processed and moved file rocketpunch/\n",
      "2024-09-12 17:35:59,451 - INFO - Start to Copy data from lake to archive\n",
      "2024-09-12 17:35:59,452 - INFO - Start to Delete data from lake\n",
      "2024-09-12 17:35:59,453 - INFO - Processed and moved file rocketpunch/data/\n",
      "2024-09-12 17:35:59,762 - INFO - Start to Copy data from lake to archive\n",
      "2024-09-12 17:35:59,763 - INFO - Start to Delete data from lake\n",
      "2024-09-12 17:35:59,764 - INFO - Processed and moved file rocketpunch/data/2024-09-12_0347.json\n",
      "2024-09-12 17:35:59,765 - INFO - Data successfully imported from bucket crawl-data-lake\n",
      "2024-09-12 17:35:59,767 - INFO - Preprocessing started\n",
      "2024-09-12 17:35:59,768 - INFO - Successfully processed data row 0\n",
      "2024-09-12 17:35:59,770 - INFO - Successfully processed data row 1\n",
      "2024-09-12 17:35:59,771 - INFO - Successfully processed data row 2\n",
      "2024-09-12 17:35:59,773 - INFO - Successfully processed data row 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    df = import_bucket()\n",
    "    # df가 있는 경우만 전처리 진행\n",
    "    if df is not None and not df.empty:\n",
    "        preprocessed_df = preprocessing(df)\n",
    "        unique_df = preprocessed_df.drop_duplicates(subset='id', keep='first')\n",
    "        upload_ids_records = utils.check_id_in_redis(logger, redis_sassion, unique_df.to_dict(orient='records'))\n",
    "        filtered_df = unique_df[unique_df['id'].isin([record['id'] for record in upload_ids_records])]\n",
    "        upload_data(filtered_df.to_dict(orient='records'))\n",
    "        # utils.upload_id_into_redis(logger, redis_sassion, upload_ids_records)\n",
    "        # utils.send_msg_to_sqs(logger, session, target_id_queue_url, \"RP\", upload_ids_records)\n",
    "        print(json.dumps(upload_ids_records)) # Airflow DAG Xcom으로 값 전달하기 위해 stdout 출력 \n",
    "    # df가 없는 경우 전처리 진행 하지 않음\n",
    "    # else:\n",
    "        # logger.info('No task for preprocessing.')\n",
    "    # sys.exit(0)\n",
    "except Exception as e:\n",
    "    # logger.error(f\"An error occurred in the main function: {e}\")\n",
    "    # sys.exit(1)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
